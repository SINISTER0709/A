{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1436057,"sourceType":"datasetVersion","datasetId":841381},{"sourceId":11512018,"sourceType":"datasetVersion","datasetId":7218818},{"sourceId":11513105,"sourceType":"datasetVersion","datasetId":7219677},{"sourceId":11515257,"sourceType":"datasetVersion","datasetId":7221330}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\n\ndataset_path = os.listdir('/kaggle/input/dataset-8class/dataset_8class/train')\n\nlabel_types = os.listdir('/kaggle/input/dataset-8class/dataset_8class/test')\nprint (label_types)  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:30:39.192713Z","iopub.execute_input":"2025-04-22T13:30:39.193261Z","iopub.status.idle":"2025-04-22T13:30:39.219437Z","shell.execute_reply.started":"2025-04-22T13:30:39.193240Z","shell.execute_reply":"2025-04-22T13:30:39.218665Z"}},"outputs":[{"name":"stdout","text":"['Typing', 'PlayingTabla', 'YoYo', 'TableTennisShot', 'PlayingGuitar', 'SkyDiving', 'Rafting', 'Skiing']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"rooms = []\n\nfor item in dataset_path:\n # Get all the file names\n all_rooms = os.listdir('/kaggle/input/dataset-8class/dataset_8class/train' + '/' +item)\n\n # Add them to the list\n for room in all_rooms:\n    rooms.append((item, str('/kaggle/input/dataset-8class/dataset_8class/train' + '/' +item) + '/' + room))\n    \n# Build a dataframe        \ntrain_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\nprint(train_df.head())\nprint(train_df.tail())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:31:10.576739Z","iopub.execute_input":"2025-04-22T13:31:10.577023Z","iopub.status.idle":"2025-04-22T13:31:10.645227Z","shell.execute_reply.started":"2025-04-22T13:31:10.577001Z","shell.execute_reply":"2025-04-22T13:31:10.644583Z"}},"outputs":[{"name":"stdout","text":"      tag                                         video_name\n0  Typing  /kaggle/input/dataset-8class/dataset_8class/tr...\n1  Typing  /kaggle/input/dataset-8class/dataset_8class/tr...\n2  Typing  /kaggle/input/dataset-8class/dataset_8class/tr...\n3  Typing  /kaggle/input/dataset-8class/dataset_8class/tr...\n4  Typing  /kaggle/input/dataset-8class/dataset_8class/tr...\n        tag                                         video_name\n828  Skiing  /kaggle/input/dataset-8class/dataset_8class/tr...\n829  Skiing  /kaggle/input/dataset-8class/dataset_8class/tr...\n830  Skiing  /kaggle/input/dataset-8class/dataset_8class/tr...\n831  Skiing  /kaggle/input/dataset-8class/dataset_8class/tr...\n832  Skiing  /kaggle/input/dataset-8class/dataset_8class/tr...\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"df = train_df.loc[:,['video_name','tag']]\ndf\ndf.to_csv('train.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:31:15.705913Z","iopub.execute_input":"2025-04-22T13:31:15.706174Z","iopub.status.idle":"2025-04-22T13:31:15.728278Z","shell.execute_reply.started":"2025-04-22T13:31:15.706157Z","shell.execute_reply":"2025-04-22T13:31:15.727674Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"dataset_path = os.listdir('/kaggle/input/dataset-8class/dataset_8class/test')\nprint(dataset_path)\n\nroom_types = os.listdir('/kaggle/input/dataset-8class/dataset_8class/test')\nprint(\"Types of activities found: \", len(dataset_path))\n\nrooms = []\n\nfor item in dataset_path:\n # Get all the file names\n all_rooms = os.listdir('/kaggle/input/dataset-8class/dataset_8class/test' + '/' +item)\n\n # Add them to the list\n for room in all_rooms:\n    rooms.append((item, str('/kaggle/input/dataset-8class/dataset_8class/test' + '/' +item) + '/' + room))\n    \n# Build a dataframe        \ntest_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\nprint(test_df.head())\nprint(test_df.tail())\n\ndf = test_df.loc[:,['video_name','tag']]\ndf\ndf.to_csv('test.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:31:50.864064Z","iopub.execute_input":"2025-04-22T13:31:50.864323Z","iopub.status.idle":"2025-04-22T13:31:50.907976Z","shell.execute_reply.started":"2025-04-22T13:31:50.864304Z","shell.execute_reply":"2025-04-22T13:31:50.907315Z"}},"outputs":[{"name":"stdout","text":"['Typing', 'PlayingTabla', 'YoYo', 'TableTennisShot', 'PlayingGuitar', 'SkyDiving', 'Rafting', 'Skiing']\nTypes of activities found:  8\n      tag                                         video_name\n0  Typing  /kaggle/input/dataset-8class/dataset_8class/te...\n1  Typing  /kaggle/input/dataset-8class/dataset_8class/te...\n2  Typing  /kaggle/input/dataset-8class/dataset_8class/te...\n3  Typing  /kaggle/input/dataset-8class/dataset_8class/te...\n4  Typing  /kaggle/input/dataset-8class/dataset_8class/te...\n        tag                                         video_name\n140  Skiing  /kaggle/input/dataset-8class/dataset_8class/te...\n141  Skiing  /kaggle/input/dataset-8class/dataset_8class/te...\n142  Skiing  /kaggle/input/dataset-8class/dataset_8class/te...\n143  Skiing  /kaggle/input/dataset-8class/dataset_8class/te...\n144  Skiing  /kaggle/input/dataset-8class/dataset_8class/te...\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"pip install git+https://github.com/tensorflow/docs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:32:02.836410Z","iopub.execute_input":"2025-04-22T13:32:02.837075Z","iopub.status.idle":"2025-04-22T13:32:12.730758Z","shell.execute_reply.started":"2025-04-22T13:32:02.837046Z","shell.execute_reply":"2025-04-22T13:32:12.729826Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/tensorflow/docs\n  Cloning https://github.com/tensorflow/docs to /tmp/pip-req-build-mb99w5tt\n  Running command git clone --filter=blob:none --quiet https://github.com/tensorflow/docs /tmp/pip-req-build-mb99w5tt\n  Resolved https://github.com/tensorflow/docs to commit a8576cef38b7182e6228d7aafca8ef51754ab9e8\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting astor (from tensorflow-docs==2025.3.6.10029)\n  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow-docs==2025.3.6.10029) (1.4.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-docs==2025.3.6.10029) (3.1.6)\nRequirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from tensorflow-docs==2025.3.6.10029) (5.10.4)\nRequirement already satisfied: protobuf>=3.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow-docs==2025.3.6.10029) (3.20.3)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from tensorflow-docs==2025.3.6.10029) (6.0.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->tensorflow-docs==2025.3.6.10029) (3.0.2)\nRequirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow-docs==2025.3.6.10029) (2.21.1)\nRequirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow-docs==2025.3.6.10029) (4.23.0)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow-docs==2025.3.6.10029) (5.7.2)\nRequirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.11/dist-packages (from nbformat->tensorflow-docs==2025.3.6.10029) (5.7.1)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2025.3.6.10029) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2025.3.6.10029) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2025.3.6.10029) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->tensorflow-docs==2025.3.6.10029) (0.22.3)\nRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->tensorflow-docs==2025.3.6.10029) (4.3.7)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat->tensorflow-docs==2025.3.6.10029) (4.13.1)\nDownloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nBuilding wheels for collected packages: tensorflow-docs\n  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for tensorflow-docs: filename=tensorflow_docs-2025.3.6.10029-py3-none-any.whl size=190416 sha256=e696f30dfa3f9735370a48b3177f9d50f38f8e84e7566944b3536ea5c9951d06\n  Stored in directory: /tmp/pip-ephem-wheel-cache-84b6k0am/wheels/34/53/89/3db54cf97ce0f0261aaab3fdc12a847ea0879d34edf373e2c5\nSuccessfully built tensorflow-docs\nInstalling collected packages: astor, tensorflow-docs\nSuccessfully installed astor-0.8.1 tensorflow-docs-2025.3.6.10029\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from tensorflow_docs.vis import embed\nfrom tensorflow import keras\nfrom imutils import paths\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport imageio\nimport cv2\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:32:17.405246Z","iopub.execute_input":"2025-04-22T13:32:17.405539Z","iopub.status.idle":"2025-04-22T13:32:31.551524Z","shell.execute_reply.started":"2025-04-22T13:32:17.405512Z","shell.execute_reply":"2025-04-22T13:32:31.550917Z"}},"outputs":[{"name":"stderr","text":"2025-04-22 13:32:18.906842: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745328739.110052      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745328739.169073      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"train_df = pd.read_csv(\"train.csv\")\ntest_df = pd.read_csv(\"test.csv\")\n\nprint(f\"Total videos for training: {len(train_df)}\")\nprint(f\"Total videos for testing: {len(test_df)}\")\n\n\ntrain_df.sample(30)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:32:36.266222Z","iopub.execute_input":"2025-04-22T13:32:36.266792Z","iopub.status.idle":"2025-04-22T13:32:36.293151Z","shell.execute_reply.started":"2025-04-22T13:32:36.266770Z","shell.execute_reply":"2025-04-22T13:32:36.292536Z"}},"outputs":[{"name":"stdout","text":"Total videos for training: 833\nTotal videos for testing: 145\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0                                         video_name  \\\n557         557  /kaggle/input/dataset-8class/dataset_8class/tr...   \n490         490  /kaggle/input/dataset-8class/dataset_8class/tr...   \n620         620  /kaggle/input/dataset-8class/dataset_8class/tr...   \n29           29  /kaggle/input/dataset-8class/dataset_8class/tr...   \n5             5  /kaggle/input/dataset-8class/dataset_8class/tr...   \n390         390  /kaggle/input/dataset-8class/dataset_8class/tr...   \n642         642  /kaggle/input/dataset-8class/dataset_8class/tr...   \n753         753  /kaggle/input/dataset-8class/dataset_8class/tr...   \n595         595  /kaggle/input/dataset-8class/dataset_8class/tr...   \n105         105  /kaggle/input/dataset-8class/dataset_8class/tr...   \n2             2  /kaggle/input/dataset-8class/dataset_8class/tr...   \n635         635  /kaggle/input/dataset-8class/dataset_8class/tr...   \n280         280  /kaggle/input/dataset-8class/dataset_8class/tr...   \n325         325  /kaggle/input/dataset-8class/dataset_8class/tr...   \n380         380  /kaggle/input/dataset-8class/dataset_8class/tr...   \n583         583  /kaggle/input/dataset-8class/dataset_8class/tr...   \n704         704  /kaggle/input/dataset-8class/dataset_8class/tr...   \n727         727  /kaggle/input/dataset-8class/dataset_8class/tr...   \n112         112  /kaggle/input/dataset-8class/dataset_8class/tr...   \n690         690  /kaggle/input/dataset-8class/dataset_8class/tr...   \n236         236  /kaggle/input/dataset-8class/dataset_8class/tr...   \n230         230  /kaggle/input/dataset-8class/dataset_8class/tr...   \n56           56  /kaggle/input/dataset-8class/dataset_8class/tr...   \n672         672  /kaggle/input/dataset-8class/dataset_8class/tr...   \n688         688  /kaggle/input/dataset-8class/dataset_8class/tr...   \n786         786  /kaggle/input/dataset-8class/dataset_8class/tr...   \n15           15  /kaggle/input/dataset-8class/dataset_8class/tr...   \n75           75  /kaggle/input/dataset-8class/dataset_8class/tr...   \n245         245  /kaggle/input/dataset-8class/dataset_8class/tr...   \n515         515  /kaggle/input/dataset-8class/dataset_8class/tr...   \n\n                 tag  \n557        SkyDiving  \n490    PlayingGuitar  \n620        SkyDiving  \n29            Typing  \n5             Typing  \n390  TableTennisShot  \n642          Rafting  \n753           Skiing  \n595        SkyDiving  \n105           Typing  \n2             Typing  \n635          Rafting  \n280             YoYo  \n325  TableTennisShot  \n380  TableTennisShot  \n583        SkyDiving  \n704          Rafting  \n727           Skiing  \n112     PlayingTabla  \n690          Rafting  \n236             YoYo  \n230             YoYo  \n56            Typing  \n672          Rafting  \n688          Rafting  \n786           Skiing  \n15            Typing  \n75            Typing  \n245             YoYo  \n515    PlayingGuitar  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>video_name</th>\n      <th>tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>557</th>\n      <td>557</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>SkyDiving</td>\n    </tr>\n    <tr>\n      <th>490</th>\n      <td>490</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>PlayingGuitar</td>\n    </tr>\n    <tr>\n      <th>620</th>\n      <td>620</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>SkyDiving</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>29</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Typing</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Typing</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>390</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>TableTennisShot</td>\n    </tr>\n    <tr>\n      <th>642</th>\n      <td>642</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Rafting</td>\n    </tr>\n    <tr>\n      <th>753</th>\n      <td>753</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Skiing</td>\n    </tr>\n    <tr>\n      <th>595</th>\n      <td>595</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>SkyDiving</td>\n    </tr>\n    <tr>\n      <th>105</th>\n      <td>105</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Typing</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Typing</td>\n    </tr>\n    <tr>\n      <th>635</th>\n      <td>635</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Rafting</td>\n    </tr>\n    <tr>\n      <th>280</th>\n      <td>280</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>YoYo</td>\n    </tr>\n    <tr>\n      <th>325</th>\n      <td>325</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>TableTennisShot</td>\n    </tr>\n    <tr>\n      <th>380</th>\n      <td>380</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>TableTennisShot</td>\n    </tr>\n    <tr>\n      <th>583</th>\n      <td>583</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>SkyDiving</td>\n    </tr>\n    <tr>\n      <th>704</th>\n      <td>704</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Rafting</td>\n    </tr>\n    <tr>\n      <th>727</th>\n      <td>727</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Skiing</td>\n    </tr>\n    <tr>\n      <th>112</th>\n      <td>112</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>PlayingTabla</td>\n    </tr>\n    <tr>\n      <th>690</th>\n      <td>690</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Rafting</td>\n    </tr>\n    <tr>\n      <th>236</th>\n      <td>236</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>YoYo</td>\n    </tr>\n    <tr>\n      <th>230</th>\n      <td>230</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>YoYo</td>\n    </tr>\n    <tr>\n      <th>56</th>\n      <td>56</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Typing</td>\n    </tr>\n    <tr>\n      <th>672</th>\n      <td>672</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Rafting</td>\n    </tr>\n    <tr>\n      <th>688</th>\n      <td>688</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Rafting</td>\n    </tr>\n    <tr>\n      <th>786</th>\n      <td>786</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Skiing</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Typing</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>75</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>Typing</td>\n    </tr>\n    <tr>\n      <th>245</th>\n      <td>245</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>YoYo</td>\n    </tr>\n    <tr>\n      <th>515</th>\n      <td>515</td>\n      <td>/kaggle/input/dataset-8class/dataset_8class/tr...</td>\n      <td>PlayingGuitar</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# The following two methods are taken from this tutorial:\n# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\nIMG_SIZE = 224\n\n\ndef crop_center_square(frame):\n    y, x = frame.shape[0:2]\n    min_dim = min(y, x)\n    start_x = (x // 2) - (min_dim // 2)\n    start_y = (y // 2) - (min_dim // 2)\n    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n\n\ndef load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n    cap = cv2.VideoCapture(path)\n    frames = []\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = crop_center_square(frame)\n            frame = cv2.resize(frame, resize)\n            frame = frame[:, :, [2, 1, 0]]\n            frames.append(frame)\n\n            if len(frames) == max_frames:\n                break\n    finally:\n        cap.release()\n    return np.array(frames)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:32:48.886495Z","iopub.execute_input":"2025-04-22T13:32:48.887117Z","iopub.status.idle":"2025-04-22T13:32:48.893364Z","shell.execute_reply.started":"2025-04-22T13:32:48.887093Z","shell.execute_reply":"2025-04-22T13:32:48.892619Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"Feature Extraction\n","metadata":{}},{"cell_type":"code","source":"def build_feature_extractor():\n    feature_extractor = keras.applications.InceptionV3(\n        weights=\"imagenet\",\n        include_top=False,\n        pooling=\"avg\",\n        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    )\n    preprocess_input = keras.applications.inception_v3.preprocess_input\n\n    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n    preprocessed = preprocess_input(inputs)\n\n    outputs = feature_extractor(preprocessed)\n    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n\n\nfeature_extractor = build_feature_extractor()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:32:57.513806Z","iopub.execute_input":"2025-04-22T13:32:57.514089Z","iopub.status.idle":"2025-04-22T13:33:02.066250Z","shell.execute_reply.started":"2025-04-22T13:32:57.514072Z","shell.execute_reply":"2025-04-22T13:33:02.065632Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1745328778.614136      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\nprint(label_processor.get_vocabulary())\n\nlabels = train_df[\"tag\"].values\nlabels = label_processor(labels[..., None]).numpy()\nlabels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:33:09.814833Z","iopub.execute_input":"2025-04-22T13:33:09.815128Z","iopub.status.idle":"2025-04-22T13:33:09.881286Z","shell.execute_reply.started":"2025-04-22T13:33:09.815108Z","shell.execute_reply":"2025-04-22T13:33:09.880509Z"}},"outputs":[{"name":"stdout","text":"['PlayingGuitar', 'PlayingTabla', 'Rafting', 'Skiing', 'SkyDiving', 'TableTennisShot', 'Typing', 'YoYo']\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"array([[6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [6],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [7],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [5],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [0],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [4],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [2],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3],\n       [3]])"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"#Define hyperparameters\n\nIMG_SIZE = 224\nBATCH_SIZE = 64\nEPOCHS = 30\n\nMAX_SEQ_LENGTH = 20\nNUM_FEATURES = 2048","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:33:56.928877Z","iopub.execute_input":"2025-04-22T13:33:56.929442Z","iopub.status.idle":"2025-04-22T13:33:56.933048Z","shell.execute_reply.started":"2025-04-22T13:33:56.929417Z","shell.execute_reply":"2025-04-22T13:33:56.932126Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def prepare_all_videos(df, root_dir):\n    num_samples = len(df)\n    video_paths = df[\"video_name\"].values.tolist()\n    labels = df[\"tag\"].values\n    labels = label_processor(labels[..., None]).numpy()\n\n    frame_masks = np.zeros((num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n    frame_features = np.zeros((num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n\n    for idx, path in enumerate(video_paths):\n        frames = load_video(os.path.join(root_dir, path))\n        video_length = frames.shape[0]\n        length = min(MAX_SEQ_LENGTH, video_length)\n\n        # Take only up to MAX_SEQ_LENGTH frames\n        frames = frames[:length]\n        \n        # Batch predict features for all selected frames\n        features = feature_extractor.predict(frames, verbose=0)\n\n        # Fill in the frame_features and frame_masks\n        frame_features[idx, :length, :] = features\n        frame_masks[idx, :length] = 1\n\n    return (frame_features, frame_masks), labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:35:03.138223Z","iopub.execute_input":"2025-04-22T13:35:03.138959Z","iopub.status.idle":"2025-04-22T13:35:03.144330Z","shell.execute_reply.started":"2025-04-22T13:35:03.138936Z","shell.execute_reply":"2025-04-22T13:35:03.143664Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def prepare_all_videos(df, root_dir):\n    num_samples = len(df)\n    video_paths = df[\"video_name\"].values.tolist()\n    \n    ##take all classlabels from train_df column named 'tag' and store in labels\n    labels = df[\"tag\"].values\n    \n    #convert classlabels to label encoding\n    labels = label_processor(labels[..., None]).numpy()\n\n    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n    # masked with padding or not.\n    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n\n    # For each video.\n    for idx, path in enumerate(video_paths):\n        # Gather all its frames and add a batch dimension.\n        frames = load_video(os.path.join(root_dir, path))\n        frames = frames[None, ...]\n\n        # Initialize placeholders to store the masks and features of the current video.\n        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n        temp_frame_features = np.zeros(\n            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n        )\n\n        # Extract features from the frames of the current video.\n        for i, batch in enumerate(frames):\n            video_length = batch.shape[0]\n            length = min(MAX_SEQ_LENGTH, video_length)\n            for j in range(length):\n                temp_frame_features[i, j, :] = feature_extractor.predict(\n                    batch[None, j, :]\n                )\n            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n\n        frame_features[idx,] = temp_frame_features.squeeze()\n        frame_masks[idx,] = temp_frame_mask.squeeze()\n\n    return (frame_features, frame_masks), labels\n\n\ntrain_data, train_labels = prepare_all_videos(train_df, \"train\")\ntest_data, test_labels = prepare_all_videos(test_df, \"test\")\n\nprint(f\"Frame features in train set: {train_data[0].shape}\")\nprint(f\"Frame masks in train set: {train_data[1].shape}\")\n\n\n\nprint(f\"train_labels in train set: {train_labels.shape}\")\n\nprint(f\"test_labels in train set: {test_labels.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_all_videos(df, root_dir):\n    num_samples = len(df)\n    video_paths = df[\"video_name\"].values.tolist()\n    labels = df[\"tag\"].values\n    labels = label_processor(labels[..., None]).numpy()\n\n    frame_masks = np.zeros((num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n    frame_features = np.zeros((num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n\n    for idx, path in enumerate(video_paths):\n        frames = load_video(os.path.join(root_dir, path))\n        video_length = frames.shape[0]\n        length = min(MAX_SEQ_LENGTH, video_length)\n\n        frames = frames[:length]\n        features = feature_extractor.predict(frames, verbose=0)\n\n        frame_features[idx, :length, :] = features\n        frame_masks[idx, :length] = 1\n\n    return (frame_features, frame_masks), labels\n\n# Use the same variable names as before\ntrain_data, train_labels = prepare_all_videos(train_df, \"train\")\ntest_data, test_labels = prepare_all_videos(test_df, \"test\")\n\nprint(f\"Frame features in train set: {train_data[0].shape}\")\nprint(f\"Frame masks in train set: {train_data[1].shape}\")\nprint(f\"train_labels in train set: {train_labels.shape}\")\nprint(f\"test_labels in train set: {test_labels.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:52:25.402567Z","iopub.execute_input":"2025-04-22T13:52:25.403315Z","iopub.status.idle":"2025-04-22T13:56:38.975250Z","shell.execute_reply.started":"2025-04-22T13:52:25.403281Z","shell.execute_reply":"2025-04-22T13:56:38.974354Z"}},"outputs":[{"name":"stdout","text":"Frame features in train set: (833, 20, 2048)\nFrame masks in train set: (833, 20)\ntrain_labels in train set: (833, 1)\ntest_labels in train set: (145, 1)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# train_data, train_labels = prepare_all_videos(train_df, \"train\")\n# test_data, test_labels = prepare_all_videos(test_df, \"test\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T13:35:45.598937Z","iopub.execute_input":"2025-04-22T13:35:45.599479Z","iopub.status.idle":"2025-04-22T13:37:55.622288Z","shell.execute_reply.started":"2025-04-22T13:35:45.599456Z","shell.execute_reply":"2025-04-22T13:37:55.621251Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745328949.740991     138 service.cc:148] XLA service 0x7e24dc094700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745328949.741794     138 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1745328950.845053     138 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1745328956.015428     138 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2376191042.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_all_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_all_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/109147357.py\u001b[0m in \u001b[0;36mprepare_all_videos\u001b[0;34m(df, root_dir)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mvideo_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/471887047.py\u001b[0m in \u001b[0;36mload_video\u001b[0;34m(path, max_frames, resize)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"print(f\"Frame features in train set: {train_data[0].shape}\")\nprint(f\"Frame masks in train set: {train_data[1].shape}\")\nprint(f\"train_labels in train set: {train_labels.shape}\")\nprint(f\"test_labels in train set: {test_labels.shape}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:00:22.584218Z","iopub.execute_input":"2025-04-22T14:00:22.584899Z","iopub.status.idle":"2025-04-22T14:00:22.589420Z","shell.execute_reply.started":"2025-04-22T14:00:22.584874Z","shell.execute_reply":"2025-04-22T14:00:22.588776Z"}},"outputs":[{"name":"stdout","text":"Frame features in train set: (833, 20, 2048)\nFrame masks in train set: (833, 20)\ntrain_labels in train set: (833, 1)\ntest_labels in train set: (145, 1)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"def get_sequence_model():\n    class_vocab = label_processor.get_vocabulary()\n\n    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n\n    x = keras.layers.GRU(64, return_sequences=True)(frame_features_input, mask=mask_input)\n    x = keras.layers.GRU(32)(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.Dense(32, activation=\"relu\")(x)\n    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n\n    rnn_model = keras.Model([frame_features_input, mask_input], output)\n\n    rnn_model.compile(\n        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n    )\n    return rnn_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:00:25.986076Z","iopub.execute_input":"2025-04-22T14:00:25.986581Z","iopub.status.idle":"2025-04-22T14:00:25.992325Z","shell.execute_reply.started":"2025-04-22T14:00:25.986554Z","shell.execute_reply":"2025-04-22T14:00:25.991501Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.applications.efficientnet import preprocess_input\n\ndef build_feature_extractor():\n    base_model = EfficientNetB0(\n        weights=\"imagenet\",\n        include_top=False,\n        pooling=\"avg\",\n        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n    )\n\n    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n    x = preprocess_input(inputs)\n    outputs = base_model(x)\n    return keras.Model(inputs, outputs, name=\"efficientnet_feature_extractor\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:00:36.457347Z","iopub.execute_input":"2025-04-22T14:00:36.457658Z","iopub.status.idle":"2025-04-22T14:00:36.462619Z","shell.execute_reply.started":"2025-04-22T14:00:36.457634Z","shell.execute_reply":"2025-04-22T14:00:36.461757Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def run_experiment():\n    filepath = \"./video_classifier.weights.h5\"  # Must end with .weights.h5\n\n    checkpoint = keras.callbacks.ModelCheckpoint(\n        filepath, save_weights_only=True, save_best_only=True, verbose=1\n    )\n\n    seq_model = get_sequence_model()\n    history = seq_model.fit(\n        [train_data[0], train_data[1]],\n        train_labels,\n        validation_split=0.2,\n        epochs=30,\n        batch_size=16,\n        callbacks=[checkpoint],\n    )\n\n    seq_model.load_weights(filepath)\n    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n\n    return history, seq_model\n\n_, sequence_model = run_experiment()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:00:41.481257Z","iopub.execute_input":"2025-04-22T14:00:41.481786Z","iopub.status.idle":"2025-04-22T14:00:58.360235Z","shell.execute_reply.started":"2025-04-22T14:00:41.481763Z","shell.execute_reply":"2025-04-22T14:00:58.359636Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30\n\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.3337 - loss: 1.8142\nEpoch 1: val_loss improved from inf to 2.93712, saving model to ./video_classifier.weights.h5\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.3524 - loss: 1.7789 - val_accuracy: 0.0000e+00 - val_loss: 2.9371\nEpoch 2/30\n\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8483 - loss: 0.6819\nEpoch 2: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8516 - loss: 0.6733 - val_accuracy: 0.2395 - val_loss: 3.0913\nEpoch 3/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9522 - loss: 0.3279\nEpoch 3: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9528 - loss: 0.3255 - val_accuracy: 0.3174 - val_loss: 3.7036\nEpoch 4/30\n\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9838 - loss: 0.1663\nEpoch 4: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9841 - loss: 0.1648 - val_accuracy: 0.3293 - val_loss: 4.2316\nEpoch 5/30\n\u001b[1m37/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9963 - loss: 0.1046\nEpoch 5: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9962 - loss: 0.1027 - val_accuracy: 0.3413 - val_loss: 4.0036\nEpoch 6/30\n\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0611\nEpoch 6: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0606 - val_accuracy: 0.3413 - val_loss: 4.7961\nEpoch 7/30\n\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9960 - loss: 0.0448\nEpoch 7: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9959 - loss: 0.0448 - val_accuracy: 0.3413 - val_loss: 5.1482\nEpoch 8/30\n\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0383\nEpoch 8: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0380 - val_accuracy: 0.3413 - val_loss: 4.9443\nEpoch 9/30\n\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0296\nEpoch 9: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0291 - val_accuracy: 0.3413 - val_loss: 5.5483\nEpoch 10/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9982 - loss: 0.0236\nEpoch 10: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9981 - loss: 0.0236 - val_accuracy: 0.3413 - val_loss: 5.4683\nEpoch 11/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0212\nEpoch 11: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0211 - val_accuracy: 0.3413 - val_loss: 5.8420\nEpoch 12/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0215\nEpoch 12: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0213 - val_accuracy: 0.3413 - val_loss: 5.8082\nEpoch 13/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0137\nEpoch 13: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0137 - val_accuracy: 0.3413 - val_loss: 6.0754\nEpoch 14/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0113\nEpoch 14: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0113 - val_accuracy: 0.3353 - val_loss: 6.4948\nEpoch 15/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0101\nEpoch 15: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 0.3353 - val_loss: 6.6446\nEpoch 16/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0083\nEpoch 16: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0083 - val_accuracy: 0.3413 - val_loss: 6.6671\nEpoch 17/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0080\nEpoch 17: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0080 - val_accuracy: 0.3293 - val_loss: 6.5832\nEpoch 18/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0063\nEpoch 18: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 0.3293 - val_loss: 6.8911\nEpoch 19/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9991 - loss: 0.0084\nEpoch 19: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9991 - loss: 0.0084 - val_accuracy: 0.3413 - val_loss: 6.9594\nEpoch 20/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0050\nEpoch 20: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0051 - val_accuracy: 0.3413 - val_loss: 6.4591\nEpoch 21/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0090\nEpoch 21: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 0.3413 - val_loss: 6.9385\nEpoch 22/30\n\u001b[1m39/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0062\nEpoch 22: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0065 - val_accuracy: 0.3413 - val_loss: 6.9209\nEpoch 23/30\n\u001b[1m38/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0077\nEpoch 23: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0076 - val_accuracy: 0.3413 - val_loss: 7.1083\nEpoch 24/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0046\nEpoch 24: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0046 - val_accuracy: 0.3413 - val_loss: 6.7977\nEpoch 25/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9999 - loss: 0.0064\nEpoch 25: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9999 - loss: 0.0069 - val_accuracy: 0.3413 - val_loss: 5.8382\nEpoch 26/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9605 - loss: 0.1318\nEpoch 26: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9602 - loss: 0.1330 - val_accuracy: 0.1916 - val_loss: 6.6340\nEpoch 27/30\n\u001b[1m41/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9514 - loss: 0.2057\nEpoch 27: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9520 - loss: 0.2043 - val_accuracy: 0.3114 - val_loss: 5.6545\nEpoch 28/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9997 - loss: 0.0286\nEpoch 28: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9996 - loss: 0.0282 - val_accuracy: 0.3054 - val_loss: 7.8735\nEpoch 29/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9968 - loss: 0.0247\nEpoch 29: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9964 - loss: 0.0263 - val_accuracy: 0.3293 - val_loss: 6.1921\nEpoch 30/30\n\u001b[1m40/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9979 - loss: 0.0284\nEpoch 30: val_loss did not improve from 2.93712\n\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9975 - loss: 0.0297 - val_accuracy: 0.3353 - val_loss: 6.3666\n\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8520 - loss: 0.8529 \nTest accuracy: 76.55%\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"def prepare_single_video(frames):\n    frames = frames[None, ...]\n    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n\n    for i, batch in enumerate(frames):\n        video_length = batch.shape[0]\n        length = min(MAX_SEQ_LENGTH, video_length)\n        for j in range(length):\n            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n\n    return frame_features, frame_mask\n\n\ndef sequence_prediction(path):\n    class_vocab = label_processor.get_vocabulary()\n\n    frames = load_video(os.path.join(\"test\", path))\n    frame_features, frame_mask = prepare_single_video(frames)\n    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n\n    for i in np.argsort(probabilities)[::-1]:\n        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n    return frames\n\ntest_video = np.random.choice(test_df[\"video_name\"].values.tolist())\nprint(f\"Test video path: {test_video}\")\n\ntest_frames = sequence_prediction(test_video)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:01:37.256900Z","iopub.execute_input":"2025-04-22T14:01:37.257172Z","iopub.status.idle":"2025-04-22T14:01:39.704251Z","shell.execute_reply.started":"2025-04-22T14:01:37.257156Z","shell.execute_reply":"2025-04-22T14:01:39.703451Z"}},"outputs":[{"name":"stdout","text":"Test video path: /kaggle/input/dataset-8class/dataset_8class/test/TableTennisShot/v_TableTennisShot_g24_c03.avi\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648ms/step\n  TableTennisShot: 57.36%\n  PlayingGuitar: 19.44%\n  SkyDiving:  8.50%\n  PlayingTabla:  7.80%\n  YoYo:  2.78%\n  Rafting:  1.77%\n  Skiing:  1.22%\n  Typing:  1.12%\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"def predict_video(path):\n    # Load and preprocess video\n    frames = load_video(path)\n    video_length = frames.shape[0]\n    length = min(MAX_SEQ_LENGTH, video_length)\n    frames = frames[:length]\n\n    # Extract features\n    features = feature_extractor.predict(frames, verbose=0)\n\n    # Prepare padded feature array\n    frame_features = np.zeros((1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n    frame_masks = np.zeros((1, MAX_SEQ_LENGTH), dtype=\"bool\")\n    frame_features[0, :length, :] = features\n    frame_masks[0, :length] = 1\n\n    # Predict\n    probabilities = sequence_model.predict([frame_features, frame_masks])[0]\n    predicted_index = np.argmax(probabilities)\n    predicted_class = label_processor.get_vocabulary()[predicted_index]\n\n    return predicted_class, probabilities\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:01:51.288921Z","iopub.execute_input":"2025-04-22T14:01:51.289524Z","iopub.status.idle":"2025-04-22T14:01:51.294757Z","shell.execute_reply.started":"2025-04-22T14:01:51.289503Z","shell.execute_reply":"2025-04-22T14:01:51.294022Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"video_path = \"/kaggle/input/dataset-8class/dataset_8class/test/SkyDiving/v_SkyDiving_g21_c01.avi\"\npredicted_class, prob = predict_video(video_path)\nprint(f\"Predicted class: {predicted_class}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:02:09.558365Z","iopub.execute_input":"2025-04-22T14:02:09.558937Z","iopub.status.idle":"2025-04-22T14:02:09.907728Z","shell.execute_reply.started":"2025-04-22T14:02:09.558914Z","shell.execute_reply":"2025-04-22T14:02:09.906935Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\nPredicted class: SkyDiving\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"video_path = \"/kaggle/input/dataset-8class/dataset_8class/test/YoYo/v_YoYo_g01_c06.avi\"\npredicted_class, prob = predict_video(video_path)\nprint(f\"Predicted class: {predicted_class}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:02:24.701009Z","iopub.execute_input":"2025-04-22T14:02:24.701558Z","iopub.status.idle":"2025-04-22T14:02:25.007976Z","shell.execute_reply.started":"2025-04-22T14:02:24.701537Z","shell.execute_reply":"2025-04-22T14:02:25.007191Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\nPredicted class: YoYo\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"sequence_model.save('kaggle/working/class8_ker.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:02:58.726666Z","iopub.execute_input":"2025-04-22T14:02:58.726968Z","iopub.status.idle":"2025-04-22T14:02:58.771001Z","shell.execute_reply.started":"2025-04-22T14:02:58.726948Z","shell.execute_reply":"2025-04-22T14:02:58.770379Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"sequence_model.save('class2_keras_savedmodel', save_format='tf')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T11:24:26.203378Z","iopub.execute_input":"2025-04-22T11:24:26.203671Z","iopub.status.idle":"2025-04-22T11:24:26.218874Z","shell.execute_reply.started":"2025-04-22T11:24:26.203619Z","shell.execute_reply":"2025-04-22T11:24:26.217862Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/1168394197.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msequence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'class2_keras_savedmodel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, zipped, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             )\n\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0;34m\"The `save_format` argument is deprecated in Keras 3. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;34m\"Please remove this argument and pass a file path with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf"],"ename":"ValueError","evalue":"The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf","output_type":"error"}],"execution_count":40},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}